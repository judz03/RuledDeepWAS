{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepCTRL Healthcare\n",
    "Este es un notebook que descompone en celdas el código de la red neuronal basada en reglas DeepCTRL para poder comprender mejor la integración de reglas dentro de una red neuronal.\n",
    "La regla asociada con esta aplicación es de **lógica asociativa**.\n",
    "En el código original (**train.py**) se importan funciones de dos módulos personalizados que contienen distintas funciones. Estos módulos son **utils_learning.py** y **utils_cardio.py**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, roc_auc_score, precision_score, recall_score, precision_recall_curve, accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.distributions.beta import Beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones de utils_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verification(out, pert_out, threshold=0.0):\n",
    "    '''\n",
    "    return the ratio of qualified samples.\n",
    "    '''\n",
    "    if isinstance(out, torch.Tensor):\n",
    "        return 1.0*torch.sum(pert_out-out < threshold) / out.shape[0]\n",
    "    else:\n",
    "        return 1.0*np.sum(pert_out-out < threshold) / out.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perturbed_input(input_tensor, pert_coeff):\n",
    "    '''\n",
    "    X = X + pert_coeff*rand*X\n",
    "    return input_tensor + input_tensor*pert_coeff*torch.rand()\n",
    "    '''\n",
    "    device = input_tensor.device\n",
    "    return input_tensor + torch.abs(input_tensor)*pert_coeff*torch.rand(input_tensor.shape, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parámetros necesarios para la ejecución de la red\n",
    "parser.add_argument('--datapath', type=str, default='data/cardio_train.csv')<br>\n",
    "parser.add_argument('--rule_threshold', type=float, default=129.5)<br>\n",
    "parser.add_argument('--src_usual_ratio', type=float, default=0.3)<br>\n",
    "parser.add_argument('--src_unusual_ratio', type=float, default=0.7)<br>\n",
    "parser.add_argument('--seed', type=int, default=42)<br>\n",
    "parser.add_argument('--device', type=str, default='cuda:0')<br>\n",
    "parser.add_argument('--target_rule_ratio', type=float, default=0.7)<br>\n",
    "parser.add_argument('--batch_size', type=int, default=32)<br>\n",
    "parser.add_argument('--train_ratio', type=float, default=0.7)<br>\n",
    "parser.add_argument('--validation_ratio', type=float, default=0.1)<br>\n",
    "parser.add_argument('--test_ratio', type=float, default=0.2)<br>\n",
    "parser.add_argument('--model_type', type=str, default='dataonly')<br>\n",
    "parser.add_argument('--input_dim_encoder', type=int, default=16)<br>\n",
    "parser.add_argument('--output_dim_encoder', type=int, default=16)<br>\n",
    "parser.add_argument('--hidden_dim_encoder', type=int, default=100)<br>\n",
    "parser.add_argument('--hidden_dim_db', type=int, default=16)<br>\n",
    "parser.add_argument('--n_layers', type=int, default=1)<br>\n",
    "parser.add_argument('--rule_ind', type=int, default=5)  <br>\n",
    "parser.add_argument('--epochs', type=int, default=1000, help='default: 1000')<br>\n",
    "parser.add_argument('--early_stopping_thld', type=int, default=10, help='default: 10')<br>\n",
    "parser.add_argument('--valid_freq', type=int, default=1, help='default: 1')<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaración manual de los parámetros para la ejecución de la red\n",
    "datapath = 'controllabledl/healthcare/data/cardio_train.csv'\n",
    "rule_threshold = 129.5\n",
    "src_usual_ratio = 0.3 # Porcentaje de los pacientes etiquetados como usuales usar para el entrenamiento\n",
    "src_unusual_ratio = 0.7 # Porcentaje de los pacientes etiquetados como inusuales a usar para el entrenamiento\n",
    "seed = 42\n",
    "device = 'cuda'\n",
    "target_rule_ratio = 0.7\n",
    "batch_size = 32\n",
    "train_ratio = 0.7\n",
    "validation_ratio = 0.1\n",
    "test_ratio = 0.2\n",
    "model_type = 'ours-beta0.1-scale0.01' # Esta línea declara qué tipo de red se utilizará, si una que aprenda solo de los datos o en conjunto con las reglas. En la siguiente celda anoto los posibles valores para esta variable.\n",
    "input_dim_encoder = 16\n",
    "output_dim_encoder = 16\n",
    "hidden_dim_encoder = 100\n",
    "hidden_dim_db = 16\n",
    "n_layers = 1\n",
    "rule_ind = 5\n",
    "epochs = 100\n",
    "early_stopping_thld = 20\n",
    "valid_freq = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estos son los distintos tipos de configuraciones que podemos probar añadiendo el argumento --model_type o en el caso de este JupyterNotebook, modificar la variable\n",
    "# model_type de la celda de arriba\n",
    "model_info = {'dataonly': {'rule': 0.0}, \n",
    "              'ours-beta1.0': {'beta': [1.0], 'scale': 1.0, 'lr': 0.001},\n",
    "              'ours-beta0.1': {'beta': [0.1], 'scale': 1.0, 'lr': 0.001},\n",
    "              'ours-beta0.1-scale0.1': {'beta': [0.1], 'scale': 0.1},\n",
    "              'ours-beta0.1-scale0.01': {'beta': [0.1], 'scale': 0.01},\n",
    "              'ours-beta0.1-scale0.05': {'beta': [0.1], 'scale': 0.05},\n",
    "              'ours-beta0.1-pert0.001': {'beta': [0.1], 'pert': 0.001},\n",
    "              'ours-beta0.1-pert0.01': {'beta': [0.1], 'pert': 0.01},\n",
    "              'ours-beta0.1-pert0.1': {'beta': [0.1], 'pert': 0.1},\n",
    "              'ours-beta0.1-pert1.0': {'beta': [0.1], 'pert': 1.0},\n",
    "             }  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcción de los bloques que componen a la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(NaiveModel, self).__init__()\n",
    "    self.net = nn.Identity()\n",
    "\n",
    "  def forward(self, x, alpha=0.0):\n",
    "    return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleEncoder(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim, hidden_dim=4):\n",
    "    super(RuleEncoder, self).__init__()\n",
    "    self.input_dim = input_dim\n",
    "    self.output_dim = output_dim\n",
    "    self.net = nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataEncoder(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim, hidden_dim=4):\n",
    "    super(DataEncoder, self).__init__()\n",
    "    self.input_dim = input_dim\n",
    "    self.output_dim = output_dim\n",
    "    self.net = nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red que integra el codificador de relgas y de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim, rule_encoder, data_encoder, hidden_dim=4, n_layers=2, merge='cat', skip=False, input_type='state'):\n",
    "    super(Net, self).__init__()\n",
    "    self.skip = skip\n",
    "    self.input_type = input_type\n",
    "    self.rule_encoder = rule_encoder\n",
    "    self.data_encoder = data_encoder\n",
    "    self.n_layers = n_layers\n",
    "    assert self.rule_encoder.input_dim == self.data_encoder.input_dim\n",
    "    assert self.rule_encoder.output_dim == self.data_encoder.output_dim\n",
    "    self.merge = merge\n",
    "    if merge == 'cat':\n",
    "      self.input_dim_decision_block = self.rule_encoder.output_dim * 2\n",
    "    elif merge == 'add':\n",
    "      self.input_dim_decision_block = self.rule_encoder.output_dim\n",
    "\n",
    "    self.net = []\n",
    "    for i in range(n_layers):\n",
    "      if i == 0:\n",
    "        in_dim = self.input_dim_decision_block\n",
    "      else:\n",
    "        in_dim = hidden_dim\n",
    "\n",
    "      if i == n_layers-1:\n",
    "        out_dim = output_dim\n",
    "      else:\n",
    "        out_dim = hidden_dim\n",
    "\n",
    "      self.net.append(nn.Linear(in_dim, out_dim))\n",
    "      if i != n_layers-1:\n",
    "        self.net.append(nn.ReLU())\n",
    "\n",
    "    self.net.append(nn.Sigmoid())\n",
    "\n",
    "    self.net = nn.Sequential(*self.net)\n",
    "\n",
    "  def get_z(self, x, alpha=0.0):\n",
    "    rule_z = self.rule_encoder(x)\n",
    "    data_z = self.data_encoder(x)\n",
    "\n",
    "    if self.merge == 'add':\n",
    "      z = alpha*rule_z + (1-alpha)*data_z\n",
    "    elif self.merge == 'cat':\n",
    "      z = torch.cat((alpha*rule_z, (1-alpha)*data_z), dim=-1)\n",
    "    elif self.merge == 'equal_cat':\n",
    "      z = torch.cat((rule_z, data_z), dim=-1)\n",
    "\n",
    "    return z\n",
    "\n",
    "  def forward(self, x, alpha=0.0):\n",
    "    # merge: cat or add\n",
    "\n",
    "    rule_z = self.rule_encoder(x)\n",
    "    data_z = self.data_encoder(x)\n",
    "\n",
    "    if self.merge == 'add':\n",
    "      z = alpha*rule_z + (1-alpha)*data_z\n",
    "    elif self.merge == 'cat':\n",
    "      z = torch.cat((alpha*rule_z, (1-alpha)*data_z), dim=-1) # Aquí ocurre la unión estocástica de las representaciones latentes de los encoders de reglas y de datos\n",
    "    elif self.merge == 'equal_cat':\n",
    "      z = torch.cat((rule_z, data_z), dim=-1)\n",
    "\n",
    "    if self.skip:\n",
    "      if self.input_type == 'seq':\n",
    "        return self.net(z) + x[:, -1, :]\n",
    "      else:\n",
    "        return self.net(z) + x    # predict delta values\n",
    "    else:\n",
    "      return self.net(z)    # predict absolute values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proceso de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de parámetros para reproducibilidad\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target class ratio:\n",
      "# of cardio=1: 34979/70000 (49.97%)\n",
      "# of cardio=0: 35021/70000 (50.03%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(os.path.join(datapath), delimiter=';')\n",
    "df = df.drop(['id'], axis=1)\n",
    "\n",
    "y = df['cardio'] # y representa la etiqueta, o ground truth\n",
    "X_raw = df.drop(['cardio'], axis=1) # X_raw representa solo los datos sin etiquetar\n",
    "\n",
    "print(\"Target class ratio:\")\n",
    "print(\"# of cardio=1: {}/{} ({:.2f}%)\".format(np.sum(y==1), len(y), 100*np.sum(y==1)/len(y)))\n",
    "print(\"# of cardio=0: {}/{} ({:.2f}%)\\n\".format(np.sum(y==0), len(y), 100*np.sum(y==0)/len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_trans = ColumnTransformer(\n",
    "[('age_norm', StandardScaler(), ['age']),\n",
    "('height_norm', StandardScaler(), ['height']),\n",
    "('weight_norm', StandardScaler(), ['weight']),\n",
    "('gender_cat', OneHotEncoder(), ['gender']),\n",
    "('ap_hi_norm', StandardScaler(), ['ap_hi']),\n",
    "('ap_lo_norm', StandardScaler(), ['ap_lo']),\n",
    "('cholesterol_cat', OneHotEncoder(), ['cholesterol']),\n",
    "('gluc_cat', OneHotEncoder(), ['gluc']),\n",
    "('smoke_cat', OneHotEncoder(), ['smoke']),\n",
    "('alco_cat', OneHotEncoder(), ['alco']),\n",
    "('active_cat', OneHotEncoder(), ['active']),\n",
    "], remainder='passthrough'\n",
    ")\n",
    "\n",
    "X = column_trans.fit_transform(X_raw)\n",
    "num_samples = X.shape[0]\n",
    "X_np = X.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- RULE\n",
    "# Rule : higher ap -> higher risk. Aquí se trata de un regla de lógica.\n",
    "rule_threshold = rule_threshold\n",
    "rule_ind = rule_ind\n",
    "rule_feature = 'ap_hi'\n",
    "\n",
    "low_ap_negative = (df[rule_feature] <= rule_threshold) & (df['cardio'] == 0)    # usual\n",
    "high_ap_positive = (df[rule_feature] > rule_threshold) & (df['cardio'] == 1)    # usual\n",
    "low_ap_positive = (df[rule_feature] <= rule_threshold) & (df['cardio'] == 1)    # unusual\n",
    "high_ap_negative = (df[rule_feature] > rule_threshold) & (df['cardio'] == 0)    # unusual\n",
    "# ---------- END OF RULE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Source dataset statistics:\n",
      "# of samples in Usual group: 6007\n",
      "# of samples in Unusual group: 14018\n",
      "Usual ratio: 30.00%\n"
     ]
    }
   ],
   "source": [
    "# Samples in Usual group\n",
    "X_usual = X[low_ap_negative | high_ap_positive]\n",
    "y_usual = y[low_ap_negative | high_ap_positive]\n",
    "y_usual = y_usual.to_numpy()\n",
    "X_usual, y_usual = shuffle(X_usual, y_usual, random_state=0)\n",
    "num_usual_samples = X_usual.shape[0]\n",
    "\n",
    "# Samples in Unusual group\n",
    "X_unusual = X[low_ap_positive | high_ap_negative]\n",
    "y_unusual = y[low_ap_positive | high_ap_negative]\n",
    "y_unusual = y_unusual.to_numpy()\n",
    "X_unusual, y_unusual = shuffle(X_unusual, y_unusual, random_state=0)\n",
    "num_unusual_samples = X_unusual.shape[0]\n",
    "\n",
    "# Build a source dataset\n",
    "src_usual_ratio = src_usual_ratio\n",
    "src_unusual_ratio = src_unusual_ratio\n",
    "num_samples_from_unusual = int(src_unusual_ratio * num_unusual_samples)\n",
    "num_samples_from_usual = int(num_samples_from_unusual * src_usual_ratio / (1-src_usual_ratio))\n",
    "\n",
    "X_src = np.concatenate((X_usual[:num_samples_from_usual], X_unusual[:num_samples_from_unusual]), axis=0)\n",
    "y_src = np.concatenate((y_usual[:num_samples_from_usual], y_unusual[:num_samples_from_unusual]), axis=0)\n",
    "print()\n",
    "print(\"Source dataset statistics:\")\n",
    "print(\"# of samples in Usual group: {}\".format(num_samples_from_usual))\n",
    "print(\"# of samples in Unusual group: {}\".format(num_samples_from_unusual))\n",
    "print(\"Usual ratio: {:.2f}%\".format(100 * num_samples_from_usual / (X_src.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción de los sets de entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: 14017 for training / 2002 for validation / 4006 for testing\n"
     ]
    }
   ],
   "source": [
    "train_ratio = train_ratio\n",
    "validation_ratio = validation_ratio\n",
    "test_ratio = test_ratio\n",
    "train_X, test_X, train_y, test_y = train_test_split(X_src, y_src, test_size=1 - train_ratio, random_state=seed)\n",
    "valid_X, test_X, valid_y, test_y = train_test_split(test_X, test_y, test_size=test_ratio / (test_ratio + validation_ratio), random_state=seed)\n",
    "\n",
    "train_X, train_y = torch.tensor(train_X, dtype=torch.float32, device=device), torch.tensor(train_y, dtype=torch.float32, device=device)\n",
    "valid_X, valid_y = torch.tensor(valid_X, dtype=torch.float32, device=device), torch.tensor(valid_y, dtype=torch.float32, device=device)\n",
    "test_X, test_y = torch.tensor(test_X, dtype=torch.float32, device=device), torch.tensor(test_y, dtype=torch.float32, device=device)\n",
    "\n",
    "batch_size = batch_size\n",
    "train_loader = DataLoader(TensorDataset(train_X, train_y), batch_size=batch_size, shuffle=True) # Aquí se crea el trainset a partir de revolver los datos en las líneas 172 a 174\n",
    "valid_loader = DataLoader(TensorDataset(valid_X, valid_y), batch_size=valid_X.shape[0])\n",
    "test_loader = DataLoader(TensorDataset(test_X, test_y), batch_size=test_X.shape[0])\n",
    "print(\"data size: {} for training / {} for validation / {} for testing\".format(len(train_X), len(valid_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Especificaciones del modelo a entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type: ours-beta0.1-scale0.01\tscale:0.01\tBeta distribution: Beta([0.1])\tlr: 0.001\t \tpert_coeff: 0.1\n"
     ]
    }
   ],
   "source": [
    "model_type = model_type\n",
    "if model_type not in model_info:\n",
    "    # default setting\n",
    "    lr = 0.001\n",
    "    pert_coeff = 0.1\n",
    "    scale = 1.0\n",
    "    beta_param = [1.0]\n",
    "    alpha_distribution = Beta(float(beta_param[0]), float(beta_param[0]))\n",
    "    model_params = {}\n",
    "\n",
    "else:\n",
    "    model_params = model_info[model_type] # Aquí se toma información para los parámetros de alpha y de influencia de las reglas\n",
    "    lr = model_params['lr'] if 'lr' in model_params else 0.001\n",
    "    pert_coeff = model_params['pert'] if 'pert' in model_params else 0.1 # Definición del coeficiente de perturbación\n",
    "    scale = model_params['scale'] if 'scale' in model_params else 1.0\n",
    "    beta_param = model_params['beta'] if 'beta' in model_params else [1.0]\n",
    "\n",
    "if len(beta_param) == 1:\n",
    "    alpha_distribution = Beta(float(beta_param[0]), float(beta_param[0]))\n",
    "elif len(beta_param) == 2:\n",
    "    alpha_distribution = Beta(float(beta_param[0]), float(beta_param[1]))\n",
    "\n",
    "print('model_type: {}\\tscale:{}\\tBeta distribution: Beta({})\\tlr: {}\\t \\tpert_coeff: {}'.format(model_type, scale, beta_param, lr, pert_coeff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaración de hiperparámetros de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_filename: controllabledl/healthcare/saved_models/cardio_ours-beta0.1-scale0.01_rule-ap_hi_src0.3-target0.3_seed42.demo.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aquellas variables que sean iguales a sí mismas (epochs = epochs) es porque se les quitó 'args.' para la modificación de parámetros desde terminal\n",
    "merge = 'cat'\n",
    "input_dim = 19\n",
    "output_dim_encoder = output_dim_encoder # Default 16\n",
    "hidden_dim_encoder = hidden_dim_encoder # Default 16\n",
    "hidden_dim_db = hidden_dim_db\n",
    "n_layers = n_layers\n",
    "output_dim = 1\n",
    "\n",
    "rule_encoder = RuleEncoder(input_dim, output_dim_encoder, hidden_dim_encoder)\n",
    "data_encoder = DataEncoder(input_dim, output_dim_encoder, hidden_dim_encoder)\n",
    "model = Net(input_dim, output_dim, rule_encoder, data_encoder, hidden_dim=hidden_dim_db, n_layers=n_layers, merge=merge).to(device)    # Not residual connection\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)        \n",
    "loss_rule_func = lambda x,y: torch.mean(F.relu(x-y))    # if x>y, penalize it.\n",
    "loss_task_func = nn.BCELoss()    # return scalar (reduction=mean)\n",
    "\n",
    "epochs = epochs\n",
    "early_stopping_thld = early_stopping_thld\n",
    "counter_early_stopping = 1\n",
    "valid_freq = valid_freq  \n",
    "\n",
    "saved_filename = 'cardio_{}_rule-{}_src{}-target{}_seed{}.demo.pt'.format(model_type, rule_feature, src_usual_ratio, src_usual_ratio, seed)\n",
    "saved_filename =  os.path.join('controllabledl/healthcare/saved_models', saved_filename)\n",
    "print('saved_filename: {}\\n'.format(saved_filename))\n",
    "best_val_loss = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proceso de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Valid] Epoch: 1 Loss: 0.658670 (alpha: 0.00)\t Loss(Task): 0.658670 Acc: 61.19\t Loss(Rule): 0.000306\t Ratio: 0.0000 best model is updated %%%%\n",
      "[Valid] Epoch: 2 Loss: 0.651448 (alpha: 0.00)\t Loss(Task): 0.651448 Acc: 61.24\t Loss(Rule): 0.000522\t Ratio: 0.0000 best model is updated %%%%\n",
      "[Valid] Epoch: 3 Loss: 0.649247 (alpha: 0.00)\t Loss(Task): 0.649247 Acc: 61.24\t Loss(Rule): 0.000696\t Ratio: 0.0005 best model is updated %%%%\n",
      "[Valid] Epoch: 4 Loss: 0.647123 (alpha: 0.00)\t Loss(Task): 0.647123 Acc: 60.84\t Loss(Rule): 0.000884\t Ratio: 0.0000 best model is updated %%%%\n",
      "[Valid] Epoch: 5 Loss: 0.647068 (alpha: 0.00)\t Loss(Task): 0.647068 Acc: 60.44\t Loss(Rule): 0.001154\t Ratio: 0.0000 best model is updated %%%%\n",
      "[Valid] Epoch: 6 Loss: 0.645463 (alpha: 0.00)\t Loss(Task): 0.645463 Acc: 61.64\t Loss(Rule): 0.001310\t Ratio: 0.0000 best model is updated %%%%\n",
      "[Valid] Epoch: 7 Loss: 0.639192 (alpha: 0.00)\t Loss(Task): 0.639192 Acc: 61.89\t Loss(Rule): 0.001598\t Ratio: 0.0000 best model is updated %%%%\n",
      "[Valid] Epoch: 8 Loss: 0.643452 (alpha: 0.00)\t Loss(Task): 0.643452 Acc: 62.24\t Loss(Rule): 0.001649\t Ratio: 0.0015(1/20)\n",
      "[Valid] Epoch: 9 Loss: 0.642241 (alpha: 0.00)\t Loss(Task): 0.642241 Acc: 61.09\t Loss(Rule): 0.001913\t Ratio: 0.0025(2/20)\n",
      "[Valid] Epoch: 10 Loss: 0.656293 (alpha: 0.00)\t Loss(Task): 0.656293 Acc: 62.19\t Loss(Rule): 0.002013\t Ratio: 0.0025(3/20)\n",
      "[Valid] Epoch: 11 Loss: 0.636079 (alpha: 0.00)\t Loss(Task): 0.636079 Acc: 62.89\t Loss(Rule): 0.002108\t Ratio: 0.0035 best model is updated %%%%\n",
      "[Valid] Epoch: 12 Loss: 0.635256 (alpha: 0.00)\t Loss(Task): 0.635256 Acc: 63.34\t Loss(Rule): 0.002538\t Ratio: 0.0045 best model is updated %%%%\n",
      "[Valid] Epoch: 13 Loss: 0.638611 (alpha: 0.00)\t Loss(Task): 0.638611 Acc: 62.49\t Loss(Rule): 0.002354\t Ratio: 0.0150(1/20)\n",
      "[Valid] Epoch: 14 Loss: 0.632850 (alpha: 0.00)\t Loss(Task): 0.632850 Acc: 63.49\t Loss(Rule): 0.002457\t Ratio: 0.0135 best model is updated %%%%\n",
      "[Valid] Epoch: 15 Loss: 0.628260 (alpha: 0.00)\t Loss(Task): 0.628260 Acc: 62.89\t Loss(Rule): 0.002531\t Ratio: 0.0160 best model is updated %%%%\n",
      "[Valid] Epoch: 16 Loss: 0.628834 (alpha: 0.00)\t Loss(Task): 0.628834 Acc: 62.44\t Loss(Rule): 0.002649\t Ratio: 0.0140(1/20)\n",
      "[Valid] Epoch: 17 Loss: 0.626858 (alpha: 0.00)\t Loss(Task): 0.626858 Acc: 63.19\t Loss(Rule): 0.002527\t Ratio: 0.0160 best model is updated %%%%\n",
      "[Valid] Epoch: 18 Loss: 0.634504 (alpha: 0.00)\t Loss(Task): 0.634504 Acc: 63.34\t Loss(Rule): 0.002432\t Ratio: 0.0385(1/20)\n",
      "[Valid] Epoch: 19 Loss: 0.627867 (alpha: 0.00)\t Loss(Task): 0.627867 Acc: 63.19\t Loss(Rule): 0.002763\t Ratio: 0.0335(2/20)\n",
      "[Valid] Epoch: 20 Loss: 0.625533 (alpha: 0.00)\t Loss(Task): 0.625533 Acc: 63.74\t Loss(Rule): 0.002679\t Ratio: 0.0305 best model is updated %%%%\n",
      "[Valid] Epoch: 21 Loss: 0.629380 (alpha: 0.00)\t Loss(Task): 0.629380 Acc: 63.04\t Loss(Rule): 0.002514\t Ratio: 0.0539(1/20)\n",
      "[Valid] Epoch: 22 Loss: 0.624412 (alpha: 0.00)\t Loss(Task): 0.624412 Acc: 62.49\t Loss(Rule): 0.002862\t Ratio: 0.0390 best model is updated %%%%\n",
      "[Valid] Epoch: 23 Loss: 0.626353 (alpha: 0.00)\t Loss(Task): 0.626353 Acc: 63.04\t Loss(Rule): 0.002623\t Ratio: 0.0709(1/20)\n",
      "[Valid] Epoch: 24 Loss: 0.621049 (alpha: 0.00)\t Loss(Task): 0.621049 Acc: 63.84\t Loss(Rule): 0.002952\t Ratio: 0.0500 best model is updated %%%%\n",
      "[Valid] Epoch: 25 Loss: 0.628036 (alpha: 0.00)\t Loss(Task): 0.628036 Acc: 64.69\t Loss(Rule): 0.002916\t Ratio: 0.0669(1/20)\n",
      "[Valid] Epoch: 26 Loss: 0.618411 (alpha: 0.00)\t Loss(Task): 0.618411 Acc: 63.74\t Loss(Rule): 0.003025\t Ratio: 0.0634 best model is updated %%%%\n",
      "[Valid] Epoch: 27 Loss: 0.621542 (alpha: 0.00)\t Loss(Task): 0.621542 Acc: 63.69\t Loss(Rule): 0.002793\t Ratio: 0.0854(1/20)\n",
      "[Valid] Epoch: 28 Loss: 0.621938 (alpha: 0.00)\t Loss(Task): 0.621938 Acc: 64.74\t Loss(Rule): 0.003072\t Ratio: 0.0889(2/20)\n",
      "[Valid] Epoch: 29 Loss: 0.626286 (alpha: 0.00)\t Loss(Task): 0.626286 Acc: 65.03\t Loss(Rule): 0.002801\t Ratio: 0.1154(3/20)\n",
      "[Valid] Epoch: 30 Loss: 0.614147 (alpha: 0.00)\t Loss(Task): 0.614147 Acc: 65.28\t Loss(Rule): 0.002861\t Ratio: 0.1214 best model is updated %%%%\n",
      "[Valid] Epoch: 31 Loss: 0.608632 (alpha: 0.00)\t Loss(Task): 0.608632 Acc: 65.33\t Loss(Rule): 0.003505\t Ratio: 0.1039 best model is updated %%%%\n",
      "[Valid] Epoch: 32 Loss: 0.629507 (alpha: 0.00)\t Loss(Task): 0.629507 Acc: 66.43\t Loss(Rule): 0.002813\t Ratio: 0.1533(1/20)\n",
      "[Valid] Epoch: 33 Loss: 0.622340 (alpha: 0.00)\t Loss(Task): 0.622340 Acc: 65.73\t Loss(Rule): 0.003204\t Ratio: 0.1484(2/20)\n",
      "[Valid] Epoch: 34 Loss: 0.612883 (alpha: 0.00)\t Loss(Task): 0.612883 Acc: 63.74\t Loss(Rule): 0.002922\t Ratio: 0.1244(3/20)\n",
      "[Valid] Epoch: 35 Loss: 0.611062 (alpha: 0.00)\t Loss(Task): 0.611062 Acc: 66.68\t Loss(Rule): 0.003270\t Ratio: 0.1284(4/20)\n",
      "[Valid] Epoch: 36 Loss: 0.617262 (alpha: 0.00)\t Loss(Task): 0.617262 Acc: 66.23\t Loss(Rule): 0.003050\t Ratio: 0.1464(5/20)\n",
      "[Valid] Epoch: 37 Loss: 0.603444 (alpha: 0.00)\t Loss(Task): 0.603444 Acc: 68.03\t Loss(Rule): 0.003159\t Ratio: 0.1613 best model is updated %%%%\n",
      "[Valid] Epoch: 38 Loss: 0.625508 (alpha: 0.00)\t Loss(Task): 0.625508 Acc: 65.73\t Loss(Rule): 0.002528\t Ratio: 0.2273(1/20)\n",
      "[Valid] Epoch: 39 Loss: 0.604001 (alpha: 0.00)\t Loss(Task): 0.604001 Acc: 68.03\t Loss(Rule): 0.003809\t Ratio: 0.2183(2/20)\n",
      "[Valid] Epoch: 40 Loss: 0.609603 (alpha: 0.00)\t Loss(Task): 0.609603 Acc: 66.88\t Loss(Rule): 0.002897\t Ratio: 0.2502(3/20)\n",
      "[Valid] Epoch: 41 Loss: 0.600298 (alpha: 0.00)\t Loss(Task): 0.600298 Acc: 69.08\t Loss(Rule): 0.003549\t Ratio: 0.1768 best model is updated %%%%\n",
      "[Valid] Epoch: 42 Loss: 0.592084 (alpha: 0.00)\t Loss(Task): 0.592084 Acc: 67.93\t Loss(Rule): 0.002818\t Ratio: 0.2143 best model is updated %%%%\n",
      "[Valid] Epoch: 43 Loss: 0.600025 (alpha: 0.00)\t Loss(Task): 0.600025 Acc: 66.58\t Loss(Rule): 0.002574\t Ratio: 0.1718(1/20)\n",
      "[Valid] Epoch: 44 Loss: 0.593585 (alpha: 0.00)\t Loss(Task): 0.593585 Acc: 67.13\t Loss(Rule): 0.002132\t Ratio: 0.2253(2/20)\n",
      "[Valid] Epoch: 45 Loss: 0.590332 (alpha: 0.00)\t Loss(Task): 0.590332 Acc: 68.78\t Loss(Rule): 0.004099\t Ratio: 0.2263 best model is updated %%%%\n",
      "[Valid] Epoch: 46 Loss: 0.599761 (alpha: 0.00)\t Loss(Task): 0.599761 Acc: 66.98\t Loss(Rule): 0.002247\t Ratio: 0.2592(1/20)\n",
      "[Valid] Epoch: 47 Loss: 0.607255 (alpha: 0.00)\t Loss(Task): 0.607255 Acc: 68.73\t Loss(Rule): 0.001856\t Ratio: 0.3726(2/20)\n",
      "[Valid] Epoch: 48 Loss: 0.595044 (alpha: 0.00)\t Loss(Task): 0.595044 Acc: 68.98\t Loss(Rule): 0.002222\t Ratio: 0.3162(3/20)\n",
      "[Valid] Epoch: 49 Loss: 0.599509 (alpha: 0.00)\t Loss(Task): 0.599509 Acc: 69.78\t Loss(Rule): 0.002884\t Ratio: 0.2832(4/20)\n",
      "[Valid] Epoch: 50 Loss: 0.612462 (alpha: 0.00)\t Loss(Task): 0.612462 Acc: 67.58\t Loss(Rule): 0.002415\t Ratio: 0.3082(5/20)\n",
      "[Valid] Epoch: 51 Loss: 0.607280 (alpha: 0.00)\t Loss(Task): 0.607280 Acc: 68.98\t Loss(Rule): 0.002155\t Ratio: 0.3526(6/20)\n",
      "[Valid] Epoch: 52 Loss: 0.591736 (alpha: 0.00)\t Loss(Task): 0.591736 Acc: 68.68\t Loss(Rule): 0.002122\t Ratio: 0.3402(7/20)\n",
      "[Valid] Epoch: 53 Loss: 0.586340 (alpha: 0.00)\t Loss(Task): 0.586340 Acc: 69.13\t Loss(Rule): 0.001520\t Ratio: 0.4775 best model is updated %%%%\n",
      "[Valid] Epoch: 54 Loss: 0.591698 (alpha: 0.00)\t Loss(Task): 0.591698 Acc: 69.28\t Loss(Rule): 0.002244\t Ratio: 0.3162(1/20)\n",
      "[Valid] Epoch: 55 Loss: 0.659409 (alpha: 0.00)\t Loss(Task): 0.659409 Acc: 65.98\t Loss(Rule): 0.001528\t Ratio: 0.4835(2/20)\n",
      "[Valid] Epoch: 56 Loss: 0.614886 (alpha: 0.00)\t Loss(Task): 0.614886 Acc: 69.08\t Loss(Rule): 0.001512\t Ratio: 0.4720(3/20)\n",
      "[Valid] Epoch: 57 Loss: 0.627016 (alpha: 0.00)\t Loss(Task): 0.627016 Acc: 68.98\t Loss(Rule): 0.001266\t Ratio: 0.4905(4/20)\n",
      "[Valid] Epoch: 58 Loss: 0.619621 (alpha: 0.00)\t Loss(Task): 0.619621 Acc: 69.63\t Loss(Rule): 0.001951\t Ratio: 0.3462(5/20)\n",
      "[Valid] Epoch: 59 Loss: 0.616307 (alpha: 0.00)\t Loss(Task): 0.616307 Acc: 69.23\t Loss(Rule): 0.001561\t Ratio: 0.4760(6/20)\n",
      "[Valid] Epoch: 60 Loss: 0.614804 (alpha: 0.00)\t Loss(Task): 0.614804 Acc: 68.88\t Loss(Rule): 0.001654\t Ratio: 0.5080(7/20)\n",
      "[Valid] Epoch: 61 Loss: 0.585012 (alpha: 0.00)\t Loss(Task): 0.585012 Acc: 69.83\t Loss(Rule): 0.001320\t Ratio: 0.4795 best model is updated %%%%\n",
      "[Valid] Epoch: 62 Loss: 0.624273 (alpha: 0.00)\t Loss(Task): 0.624273 Acc: 70.58\t Loss(Rule): 0.002103\t Ratio: 0.3691(1/20)\n",
      "[Valid] Epoch: 63 Loss: 0.626337 (alpha: 0.00)\t Loss(Task): 0.626337 Acc: 68.93\t Loss(Rule): 0.001403\t Ratio: 0.5060(2/20)\n",
      "[Valid] Epoch: 64 Loss: 0.622650 (alpha: 0.00)\t Loss(Task): 0.622650 Acc: 68.98\t Loss(Rule): 0.001303\t Ratio: 0.4940(3/20)\n",
      "[Valid] Epoch: 65 Loss: 0.602606 (alpha: 0.00)\t Loss(Task): 0.602606 Acc: 69.38\t Loss(Rule): 0.001566\t Ratio: 0.4216(4/20)\n",
      "[Valid] Epoch: 66 Loss: 0.587063 (alpha: 0.00)\t Loss(Task): 0.587063 Acc: 70.98\t Loss(Rule): 0.002918\t Ratio: 0.5160(5/20)\n",
      "[Valid] Epoch: 67 Loss: 0.640286 (alpha: 0.00)\t Loss(Task): 0.640286 Acc: 69.83\t Loss(Rule): 0.001380\t Ratio: 0.5265(6/20)\n",
      "[Valid] Epoch: 68 Loss: 0.632291 (alpha: 0.00)\t Loss(Task): 0.632291 Acc: 69.58\t Loss(Rule): 0.001237\t Ratio: 0.6074(7/20)\n",
      "[Valid] Epoch: 69 Loss: 0.640556 (alpha: 0.00)\t Loss(Task): 0.640556 Acc: 69.08\t Loss(Rule): 0.001436\t Ratio: 0.4061(8/20)\n",
      "[Valid] Epoch: 70 Loss: 0.650783 (alpha: 0.00)\t Loss(Task): 0.650783 Acc: 68.33\t Loss(Rule): 0.001948\t Ratio: 0.3427(9/20)\n",
      "[Valid] Epoch: 71 Loss: 0.628513 (alpha: 0.00)\t Loss(Task): 0.628513 Acc: 69.93\t Loss(Rule): 0.001703\t Ratio: 0.3746(10/20)\n",
      "[Valid] Epoch: 72 Loss: 0.638858 (alpha: 0.00)\t Loss(Task): 0.638858 Acc: 69.08\t Loss(Rule): 0.001754\t Ratio: 0.4311(11/20)\n",
      "[Valid] Epoch: 73 Loss: 0.628769 (alpha: 0.00)\t Loss(Task): 0.628769 Acc: 69.43\t Loss(Rule): 0.001678\t Ratio: 0.3851(12/20)\n",
      "[Valid] Epoch: 74 Loss: 0.597089 (alpha: 0.00)\t Loss(Task): 0.597089 Acc: 70.13\t Loss(Rule): 0.001214\t Ratio: 0.6244(13/20)\n",
      "[Valid] Epoch: 75 Loss: 0.596707 (alpha: 0.00)\t Loss(Task): 0.596707 Acc: 68.53\t Loss(Rule): 0.001245\t Ratio: 0.5115(14/20)\n",
      "[Valid] Epoch: 76 Loss: 0.592599 (alpha: 0.00)\t Loss(Task): 0.592599 Acc: 69.38\t Loss(Rule): 0.001447\t Ratio: 0.4970(15/20)\n",
      "[Valid] Epoch: 77 Loss: 0.596257 (alpha: 0.00)\t Loss(Task): 0.596257 Acc: 69.83\t Loss(Rule): 0.001130\t Ratio: 0.5345(16/20)\n",
      "[Valid] Epoch: 78 Loss: 0.602093 (alpha: 0.00)\t Loss(Task): 0.602093 Acc: 70.33\t Loss(Rule): 0.001333\t Ratio: 0.6159(17/20)\n",
      "[Valid] Epoch: 79 Loss: 0.606449 (alpha: 0.00)\t Loss(Task): 0.606449 Acc: 70.23\t Loss(Rule): 0.001828\t Ratio: 0.5734(18/20)\n",
      "[Valid] Epoch: 80 Loss: 0.608455 (alpha: 0.00)\t Loss(Task): 0.608455 Acc: 69.93\t Loss(Rule): 0.001482\t Ratio: 0.5639(19/20)\n",
      "[Valid] Epoch: 81 Loss: 0.621631 (alpha: 0.00)\t Loss(Task): 0.621631 Acc: 69.78\t Loss(Rule): 0.001641\t Ratio: 0.3526(20/20)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    for batch_train_x, batch_train_y in train_loader:\n",
    "        batch_train_y = batch_train_y.unsqueeze(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if model_type.startswith('dataonly'):\n",
    "            alpha = 0.0\n",
    "        elif model_type.startswith('ruleonly'):\n",
    "            alpha = 1.0\n",
    "        elif model_type.startswith('ours'):\n",
    "            alpha = alpha_distribution.sample().item()\n",
    "\n",
    "        # stable output\n",
    "        output = model(batch_train_x, alpha=alpha)\n",
    "        loss_task = loss_task_func(output, batch_train_y)\n",
    "\n",
    "        # perturbed input and its output\n",
    "        pert_batch_train_x = batch_train_x.detach().clone()\n",
    "        pert_batch_train_x[:,rule_ind] = get_perturbed_input(pert_batch_train_x[:,rule_ind], pert_coeff) # Necesita de la función get_perturbed_input en el módulo\n",
    "        # utils_learning.py\n",
    "        pert_output = model(pert_batch_train_x, alpha=alpha)\n",
    "\n",
    "        loss_rule = loss_rule_func(output, pert_output)    # output should be less than pert_output\n",
    "\n",
    "        loss = alpha * loss_rule + scale * (1 - alpha) * loss_task\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    if epoch % valid_freq == 0:\n",
    "        model.eval()\n",
    "        if  model_type.startswith('ruleonly'):\n",
    "            alpha = 1.0\n",
    "        else:\n",
    "            alpha = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_x, val_y in valid_loader:\n",
    "                val_y = val_y.unsqueeze(-1)\n",
    "\n",
    "                output = model(val_x, alpha=alpha)\n",
    "                val_loss_task = loss_task_func(output, val_y).item()\n",
    "\n",
    "                # perturbed input and its output\n",
    "                pert_val_x = val_x.detach().clone()\n",
    "                pert_val_x[:,rule_ind] = get_perturbed_input(pert_val_x[:,rule_ind], pert_coeff)\n",
    "                pert_output = model(pert_val_x, alpha=alpha)    # \\hat{y}_{p}    predicted sales from perturbed input\n",
    "\n",
    "                val_loss_rule = loss_rule_func(output, pert_output).item()\n",
    "                val_ratio = verification(pert_output, output, threshold=0.0).item()\n",
    "\n",
    "                val_loss = val_loss_task\n",
    "\n",
    "                y_true = val_y.cpu().numpy()\n",
    "                y_score = output.cpu().numpy()\n",
    "                y_pred = np.round(y_score)\n",
    "                val_acc = 100 * accuracy_score(y_true, y_pred)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            counter_early_stopping = 1\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state_dict = deepcopy(model.state_dict())\n",
    "            print('[Valid] Epoch: {} Loss: {:.6f} (alpha: {:.2f})\\t Loss(Task): {:.6f} Acc: {:.2f}\\t Loss(Rule): {:.6f}\\t Ratio: {:.4f} best model is updated %%%%'\n",
    "                .format(epoch, best_val_loss, alpha, val_loss_task, val_acc, val_loss_rule, val_ratio))\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': best_model_state_dict,\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_val_loss\n",
    "            }, saved_filename)\n",
    "        else:\n",
    "            print('[Valid] Epoch: {} Loss: {:.6f} (alpha: {:.2f})\\t Loss(Task): {:.6f} Acc: {:.2f}\\t Loss(Rule): {:.6f}\\t Ratio: {:.4f}({}/{})'\n",
    "                .format(epoch, val_loss, alpha, val_loss_task, val_acc, val_loss_rule, val_ratio, counter_early_stopping, early_stopping_thld))\n",
    "            if counter_early_stopping >= early_stopping_thld:\n",
    "                break\n",
    "            else:\n",
    "                counter_early_stopping += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model loss: 0.585012\t at epoch: 61\n",
      "\n",
      "[Test] Average loss: 0.59583944\n",
      "\n",
      "[Test] Average loss: 0.59583944 (alpha:0.0)\n",
      "[Test] Accuracy: 0.7002 (alpha:0.0)\n",
      "[Test] Ratio of verified predictions: 0.498502 (alpha:0.0)\n",
      "\n",
      "[Test] Average loss: 0.59789699 (alpha:0.1)\n",
      "[Test] Accuracy: 0.6912 (alpha:0.1)\n",
      "[Test] Ratio of verified predictions: 0.575886 (alpha:0.1)\n",
      "\n",
      "[Test] Average loss: 0.60234100 (alpha:0.2)\n",
      "[Test] Accuracy: 0.6882 (alpha:0.2)\n",
      "[Test] Ratio of verified predictions: 0.627309 (alpha:0.2)\n",
      "\n",
      "[Test] Average loss: 0.60927165 (alpha:0.3)\n",
      "[Test] Accuracy: 0.6767 (alpha:0.3)\n",
      "[Test] Ratio of verified predictions: 0.684723 (alpha:0.3)\n",
      "\n",
      "[Test] Average loss: 0.61878848 (alpha:0.4)\n",
      "[Test] Accuracy: 0.6658 (alpha:0.4)\n",
      "[Test] Ratio of verified predictions: 0.739391 (alpha:0.4)\n",
      "\n",
      "[Test] Average loss: 0.63099283 (alpha:0.5)\n",
      "[Test] Accuracy: 0.6528 (alpha:0.5)\n",
      "[Test] Ratio of verified predictions: 0.779581 (alpha:0.5)\n",
      "\n",
      "[Test] Average loss: 0.64598787 (alpha:0.6)\n",
      "[Test] Accuracy: 0.6313 (alpha:0.6)\n",
      "[Test] Ratio of verified predictions: 0.815027 (alpha:0.6)\n",
      "\n",
      "[Test] Average loss: 0.66387028 (alpha:0.7)\n",
      "[Test] Accuracy: 0.6126 (alpha:0.7)\n",
      "[Test] Ratio of verified predictions: 0.863455 (alpha:0.7)\n",
      "\n",
      "[Test] Average loss: 0.68469661 (alpha:0.8)\n",
      "[Test] Accuracy: 0.5954 (alpha:0.8)\n",
      "[Test] Ratio of verified predictions: 0.929106 (alpha:0.8)\n",
      "\n",
      "[Test] Average loss: 0.70843965 (alpha:0.9)\n",
      "[Test] Accuracy: 0.5806 (alpha:0.9)\n",
      "[Test] Ratio of verified predictions: 0.993759 (alpha:0.9)\n",
      "\n",
      "[Test] Average loss: 0.73501074 (alpha:1.0)\n",
      "[Test] Accuracy: 0.5654 (alpha:1.0)\n",
      "[Test] Ratio of verified predictions: 0.998502 (alpha:1.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "rule_encoder = RuleEncoder(input_dim, output_dim_encoder, hidden_dim_encoder)\n",
    "data_encoder = DataEncoder(input_dim, output_dim_encoder, hidden_dim_encoder)\n",
    "model_eval = Net(input_dim, output_dim, rule_encoder, data_encoder, hidden_dim=hidden_dim_db, n_layers=n_layers, merge=merge).to(device)    # Not residual connection\n",
    "\n",
    "# Aquí es donde se carga el modelo con mejor rendimiento obtenido durante el entrenamiento\n",
    "checkpoint = torch.load(saved_filename) \n",
    "model_eval.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(\"best model loss: {:.6f}\\t at epoch: {}\".format(checkpoint['loss'], checkpoint['epoch']))\n",
    "\n",
    "model_eval.eval()\n",
    "with torch.no_grad():\n",
    "   for te_x, te_y in test_loader:\n",
    "       te_y = te_y.unsqueeze(-1)\n",
    "\n",
    "output = model_eval(te_x, alpha=0.0)\n",
    "test_loss_task = loss_task_func(output, te_y).item()\n",
    "print('\\n[Test] Average loss: {:.8f}\\n'.format(test_loss_task))\n",
    "\n",
    "model_eval.eval()\n",
    "alphas = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "# perturbed input and its output\n",
    "pert_test_x = te_x.detach().clone()\n",
    "pert_test_x[:,rule_ind] = get_perturbed_input(pert_test_x[:,rule_ind], pert_coeff)\n",
    "for alpha in alphas:\n",
    "    model_eval.eval()\n",
    "    with torch.no_grad():\n",
    "      for te_x, te_y in test_loader:\n",
    "        te_y = te_y.unsqueeze(-1)\n",
    "\n",
    "      if model_type.startswith('dataonly'):\n",
    "        output = model_eval(te_x, alpha=0.0)\n",
    "      elif model_type.startswith('ours'):\n",
    "        output = model_eval(te_x, alpha=alpha)\n",
    "      elif model_type.startswith('ruleonly'):\n",
    "        output = model_eval(te_x, alpha=1.0)\n",
    "\n",
    "      test_loss_task = loss_task_func(output, te_y).item()\n",
    "\n",
    "      if model_type.startswith('dataonly'):\n",
    "        pert_output = model_eval(pert_test_x, alpha=0.0)\n",
    "      elif model_type.startswith('ours'):\n",
    "        pert_output = model_eval(pert_test_x, alpha=alpha)\n",
    "      elif model_type.startswith('ruleonly'):\n",
    "        pert_output = model_eval(pert_test_x, alpha=1.0)\n",
    "\n",
    "      test_ratio = verification(pert_output, output, threshold=0.0).item()\n",
    "\n",
    "      y_true = te_y.cpu().numpy()\n",
    "      y_score = output.cpu().numpy()\n",
    "      y_pred = np.round(y_score)\n",
    "      test_acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    print('[Test] Average loss: {:.8f} (alpha:{})'.format(test_loss_task, alpha))\n",
    "    print('[Test] Accuracy: {:.4f} (alpha:{})'.format(test_acc, alpha))\n",
    "    print(\"[Test] Ratio of verified predictions: {:.6f} (alpha:{})\".format(test_ratio, alpha))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para graficar rendimiento (no completado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_true, y_pred, y_score):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    return acc, prec, recall, fpr, tpr, roc_auc\n",
    "\n",
    "def get_correct_results(out, label_Y):\n",
    "    y_pred_tag = torch.round(out)    # Binary label\n",
    "    return (y_pred_tag == label_Y).sum().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, prec, recall,fpr, tpr, roc_auc = get_metrics(y_true,y_pred,y_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8de06eb06924f2381967eb3427f566649adc4163853af4df37a480096c9b0f8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
